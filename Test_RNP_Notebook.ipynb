{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "642f1d31-7f77-4d42-950a-16ae21643742",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Defyning variables to connect with db\n",
    "driver = \"org.postgresql.Driver\"\n",
    "database_host = \"psql-mock-database-cloud.postgres.database.azure.com\"\n",
    "database_port = \"5432\" # update if you use a non-default port\n",
    "database_name = \"ecom1689953593876hxincshnhoiqouxc\"\n",
    "user = \"leimsgqkzowufdayhgewwwrf@psql-mock-database-cloud\"\n",
    "password = \"dujntukagjbgkgkuejjjjcik\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43dba527-2f61-4112-982c-e6a310654dc6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# uri connection with db\n",
    "url = f\"jdbc:postgresql://{database_host}:{database_port}/{database_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed0e380c-dc66-4983-b987-7a0fee452839",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.functions import col, asc,desc, sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b94d6199-0b03-4606-96d3-28e4792ca92d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Questão 1\n",
    "\n",
    "# bringing table orders\n",
    "table = \"orders\"\n",
    "df_orders = (spark.read\n",
    "  .format(\"jdbc\")\n",
    "  .option(\"driver\", driver)\n",
    "  .option(\"url\", url)\n",
    "  .option(\"dbtable\", table)\n",
    "  .option(\"user\", user)\n",
    "  .option(\"password\", password)\n",
    "  .load()\n",
    ")\n",
    "\n",
    "# bringing table orderdetails\n",
    "table = \"orderdetails\"\n",
    "df_orderdetails = (spark.read\n",
    "  .format(\"jdbc\")\n",
    "  .option(\"driver\", driver)\n",
    "  .option(\"url\", url)\n",
    "  .option(\"dbtable\", table)\n",
    "  .option(\"user\", user)\n",
    "  .option(\"password\", password)\n",
    "  .load()\n",
    ")\n",
    "\n",
    "# bringing table customers\n",
    "table = \"customers\"\n",
    "df_customers = (spark.read\n",
    "  .format(\"jdbc\")\n",
    "  .option(\"driver\", driver)\n",
    "  .option(\"url\", url)\n",
    "  .option(\"dbtable\", table)\n",
    "  .option(\"user\", user)\n",
    "  .option(\"password\", password)\n",
    "  .load()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e402aa3-a12a-4a40-bacf-41d00d652e13",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------+\n|    country|sum(quantity_ordered)|\n+-----------+---------------------+\n|      Spain|                  605|\n|New Zealand|                  596|\n|     Sweden|                  550|\n|        USA|                  454|\n|         UK|                  429|\n+-----------+---------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_question_1 = df_orders.join(df_orderdetails, on='order_number',how='inner')\n",
    "df_question_1 = df_question_1.join(df_customers, how='inner',on='customer_number')\n",
    "df_question_1 = df_question_1.filter(df_question_1[\"status\"]=='Cancelled')\n",
    "df_question_1 = df_question_1.select('country','quantity_ordered').groupBy('country').sum('quantity_ordered')\n",
    "df_question_1.orderBy(col('sum(quantity_ordered)').desc()).show()\n",
    "# RESPOSTA: Espanha foi o país com maior quantidade de items cancelados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "742abe70-431d-4063-9b88-06d3a8738e99",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Questão 2\n",
    "\n",
    "# bringing table orders\n",
    "table = \"orders\"\n",
    "df_orders = (spark.read\n",
    "  .format(\"jdbc\")\n",
    "  .option(\"driver\", driver)\n",
    "  .option(\"url\", url)\n",
    "  .option(\"dbtable\", table)\n",
    "  .option(\"user\", user)\n",
    "  .option(\"password\", password)\n",
    "  .load()\n",
    ")\n",
    "\n",
    "# bringing table orderdetails\n",
    "table = \"orderdetails\"\n",
    "df_orderdetails = (spark.read\n",
    "  .format(\"jdbc\")\n",
    "  .option(\"driver\", driver)\n",
    "  .option(\"url\", url)\n",
    "  .option(\"dbtable\", table)\n",
    "  .option(\"user\", user)\n",
    "  .option(\"password\", password)\n",
    "  .load()\n",
    ")\n",
    "\n",
    "# bringing table products\n",
    "table = \"products\"\n",
    "df_products = (spark.read\n",
    "  .format(\"jdbc\")\n",
    "  .option(\"driver\", driver)\n",
    "  .option(\"url\", url)\n",
    "  .option(\"dbtable\", table)\n",
    "  .option(\"user\", user)\n",
    "  .option(\"password\", password)\n",
    "  .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "901c264a-c597-436c-b228-5e5e4741ed1c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_question_2 = df_orders.join(df_orderdetails, on='order_number',how='inner')\n",
    "df_question_2 = df_question_2.join(df_products, how='inner',on='product_code')\n",
    "df_question_2 = df_question_2.filter((col('order_date').startswith('2005')) & (col('status')=='Shipped'))\n",
    "grouped = df_question_2.select('product_code','product_name','price_each','quantity_ordered').groupBy('product_code').sum('quantity_ordered').orderBy(col('sum(quantity_ordered)').desc())\n",
    "grouped.select('product_code')\n",
    "#S18_3232 is the best seller\n",
    "df_question_2 = df_question_2.filter(col('product_code')=='S18_3232').select('product_code','product_name','price_each','quantity_ordered')\n",
    "df_question_2 = df_question_2.withColumn('total', df_question_2.price_each * df_question_2.quantity_ordered)\n",
    "df_question_2 = df_question_2.select(sum(df_question_2.total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00dc60f5-6b11-48b0-8905-0435214c0283",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n|sum(total)|\n+----------+\n|  52978.28|\n+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df_question_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df804e82-0195-4e05-bd5c-f9793587baad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Questão 3\n",
    "\n",
    "# bringing table offices\n",
    "table = \"offices\"\n",
    "df_offices = (spark.read\n",
    "  .format(\"jdbc\")\n",
    "  .option(\"driver\", driver)\n",
    "  .option(\"url\", url)\n",
    "  .option(\"dbtable\", table)\n",
    "  .option(\"user\", user)\n",
    "  .option(\"password\", password)\n",
    "  .load()\n",
    ")\n",
    "\n",
    "# bringing table employees\n",
    "table = \"employees\"\n",
    "df_employees = (spark.read\n",
    "  .format(\"jdbc\")\n",
    "  .option(\"driver\", driver)\n",
    "  .option(\"url\", url)\n",
    "  .option(\"dbtable\", table)\n",
    "  .option(\"user\", user)\n",
    "  .option(\"password\", password)\n",
    "  .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cfe9175-eb55-481a-b8ec-9c7292138dde",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3910a90-b74c-4294-be30-f2d5da5261c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------------------------------+\n|first_name|last_name|regexp_replace(email, (^[^@]*), *, 1)|\n+----------+---------+-------------------------------------+\n|      Mami|    Nishi|                 *@classicmodelcar...|\n|   Yoshimi|     Kato|                 *@classicmodelcar...|\n+----------+---------+-------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_question_3 = df_offices.join(df_employees, on='office_code',how='inner')\n",
    "df_question_3 = df_question_3.filter(df_question_3[\"country\"]=='Japan').select('first_name','last_name','email')\n",
    "df_question_3 = df_question_3.select('first_name','last_name',regexp_replace('email','(^[^@]*)','*')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "331ad231-731b-4d1f-83c2-54d85a3a22d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2510085955115330>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m df_question_1\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelta\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39msaveAsTable(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mquestion1\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
       "\u001B[1;32m      2\u001B[0m df_question_2\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelta\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39msaveAsTable(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mquestion2\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
       "\u001B[1;32m      3\u001B[0m df_question_3\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelta\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39msaveAsTable(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mquestion3\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1520\u001B[0m, in \u001B[0;36mDataFrameWriter.saveAsTable\u001B[0;34m(self, name, format, mode, partitionBy, **options)\u001B[0m\n",
       "\u001B[1;32m   1518\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mformat\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m   1519\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mformat\u001B[39m)\n",
       "\u001B[0;32m-> 1520\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msaveAsTable\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: A schema mismatch detected when writing to the Delta table (Table ID: f46b9b10-a265-42e2-9f57-7e12b5bf1d50).\n",
       "To enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n",
       "'.option(\"mergeSchema\", \"true\")'.\n",
       "For other operations, set the session configuration\n",
       "spark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\n",
       "specific to the operation for details.\n",
       "\n",
       "Table schema:\n",
       "root\n",
       "-- order_number: integer (nullable = true)\n",
       "-- order_date: date (nullable = true)\n",
       "-- required_date: date (nullable = true)\n",
       "-- shipped_date: date (nullable = true)\n",
       "-- status: string (nullable = true)\n",
       "-- comments: string (nullable = true)\n",
       "-- customer_number: integer (nullable = true)\n",
       "-- product_code: string (nullable = true)\n",
       "-- quantity_ordered: integer (nullable = true)\n",
       "-- price_each: decimal(5,2) (nullable = true)\n",
       "-- order_line_number: short (nullable = true)\n",
       "-- customer_name: string (nullable = true)\n",
       "-- contact_last_name: string (nullable = true)\n",
       "-- contact_first_name: string (nullable = true)\n",
       "-- phone: string (nullable = true)\n",
       "-- address_line1: string (nullable = true)\n",
       "-- address_line2: void (nullable = true)\n",
       "-- city: string (nullable = true)\n",
       "-- state: string (nullable = true)\n",
       "-- postal_code: string (nullable = true)\n",
       "-- country: string (nullable = true)\n",
       "-- sales_rep_employee_number: double (nullable = true)\n",
       "-- credit_limit: decimal(8,2) (nullable = true)\n",
       "\n",
       "\n",
       "Data schema:\n",
       "root\n",
       "-- country: string (nullable = true)\n",
       "-- sum(quantity_ordered): long (nullable = true)\n",
       "\n",
       "         \n",
       "To overwrite your schema or change partitioning, please set:\n",
       "'.option(\"overwriteSchema\", \"true\")'.\n",
       "\n",
       "Note that the schema can't be overwritten when using\n",
       "'replaceWhere'.\n",
       "         "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-2510085955115330>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m df_question_1\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelta\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39msaveAsTable(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mquestion1\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      2\u001B[0m df_question_2\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelta\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39msaveAsTable(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mquestion2\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      3\u001B[0m df_question_3\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelta\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39msaveAsTable(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mquestion3\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1520\u001B[0m, in \u001B[0;36mDataFrameWriter.saveAsTable\u001B[0;34m(self, name, format, mode, partitionBy, **options)\u001B[0m\n\u001B[1;32m   1518\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mformat\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1519\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mformat\u001B[39m)\n\u001B[0;32m-> 1520\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msaveAsTable\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: A schema mismatch detected when writing to the Delta table (Table ID: f46b9b10-a265-42e2-9f57-7e12b5bf1d50).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- order_number: integer (nullable = true)\n-- order_date: date (nullable = true)\n-- required_date: date (nullable = true)\n-- shipped_date: date (nullable = true)\n-- status: string (nullable = true)\n-- comments: string (nullable = true)\n-- customer_number: integer (nullable = true)\n-- product_code: string (nullable = true)\n-- quantity_ordered: integer (nullable = true)\n-- price_each: decimal(5,2) (nullable = true)\n-- order_line_number: short (nullable = true)\n-- customer_name: string (nullable = true)\n-- contact_last_name: string (nullable = true)\n-- contact_first_name: string (nullable = true)\n-- phone: string (nullable = true)\n-- address_line1: string (nullable = true)\n-- address_line2: void (nullable = true)\n-- city: string (nullable = true)\n-- state: string (nullable = true)\n-- postal_code: string (nullable = true)\n-- country: string (nullable = true)\n-- sales_rep_employee_number: double (nullable = true)\n-- credit_limit: decimal(8,2) (nullable = true)\n\n\nData schema:\nroot\n-- country: string (nullable = true)\n-- sum(quantity_ordered): long (nullable = true)\n\n         \nTo overwrite your schema or change partitioning, please set:\n'.option(\"overwriteSchema\", \"true\")'.\n\nNote that the schema can't be overwritten when using\n'replaceWhere'.\n         ",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: A schema mismatch detected when writing to the Delta table (Table ID: f46b9b10-a265-42e2-9f57-7e12b5bf1d50).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- order_number: integer (nullable = true)\n-- order_date: date (nullable = true)\n-- required_date: date (nullable = true)\n-- shipped_date: date (nullable = true)\n-- status: string (nullable = true)\n-- comments: string (nullable = true)\n-- customer_number: integer (nullable = true)\n-- product_code: string (nullable = true)\n-- quantity_ordered: integer (nullable = true)\n-- price_each: decimal(5,2) (nullable = true)\n-- order_line_number: short (nullable = true)\n-- customer_name: string (nullable = true)\n-- contact_last_name: string (nullable = true)\n-- contact_first_name: string (nullable = true)\n-- phone: string (nullable = true)\n-- address_line1: string (nullable = true)\n-- address_line2: void (nullable = true)\n-- city: string (nullable = true)\n-- state: string (nullable = true)\n-- postal_code: string (nullable = true)\n-- country: string (nullable = true)\n-- sales_rep_employee_number: double (nullable = true)\n-- credit_limit: decimal(8,2) (nullable = true)\n\n\nData schema:\nroot\n-- country: string (nullable = true)\n-- sum(quantity_ordered): long (nullable = true)\n\n         \nTo overwrite your schema or change partitioning, please set:\n'.option(\"overwriteSchema\", \"true\")'.\n\nNote that the schema can't be overwritten when using\n'replaceWhere'.\n         ",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_question_1.write.mode(\"overwrite\").format(\"delta\").saveAsTable('question1')\n",
    "df_question_2.write.mode(\"overwrite\").format(\"delta\").saveAsTable('question2')\n",
    "df_question_3.write.mode(\"overwrite\").format(\"delta\").saveAsTable('question3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a253612f-7f69-4a56-932b-bf8d1137ec2d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Test RNP",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
